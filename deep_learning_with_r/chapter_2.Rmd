---
title: '2장 : 시작하기 전에, 신경망의 수학적 빌딩 블록'
output: html_document
date: "2025-11-18"

header-includes:
  - \newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
---
# 2장에서 다루는 내용

- 신경망의 첫번째 예제

- 텐서 및 텐서 연산

- 역전파 및 경사 하강을 통해 신경망을 학습하는 방법

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 2.1 신경망 둘러보기

케라스 R 패키지를 사용한 손글씨 숫자를 분류하는 방법 
- 열 개 숫자로 된 손글씨(28 \times 28 픽셀)의 회색 음영 이미지를 열 개 범주(0~9)로 분류
- MNIST : 1980년대 미국 국립 표준 기술 연구소(NIST)에서 구성한 6만 개 훈련 이미지와 1만 개 테스트 이미지 집합

```{r}
# install.packages("reticulate")
# library(reticulate)
# use_condaenv("r-tf-env",
#              conda = 'C:/Users/PC/anaconda3/condabin/conda.bat',
#              required = TRUE)
# install.packages("tensorflow")
# install.packages("keras3")
library(tensorflow)
library(keras)

tf$config$list_physical_devices("GPU")  # GPU 인식 확인
```

```{r}
library(tensorflow)

tf$config$list_physical_devices()           # 전체 디바이스
tf$config$list_physical_devices("CPU")      # CPU
tf$config$list_physical_devices("GPU")      # GPU
tf$`__version__`
```

```{r}
library(tensorflow)

# 어떤 디바이스를 쓰는지 확인
tf$config$list_logical_devices()
```

### 목록 2.1 케라스에 내장된 MNIST 데이터셋 가져오기

```{r}
library(keras)
mnist = dataset_mnist()
train_images = mnist$train$x
train_labels = mnist$train$y
test_images = mnist$test$x
test_labels = mnist$test$y
```

```{r, echo=FALSE}
str(train_images)
str(train_labels)

str(test_images)
str(test_labels)
```

### 목록 2.2 신경망 아키텍처

```{r}
network = keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
  layer_dense(units = 10, activation = "softmax")
```

### 목록 2.3 컴파일 단계

```{r}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

### 목록 2.4 이미지 데이터 준비하기

```{r}
train_images = array_reshape(train_images, c(60000, 28 * 28))
train_images = train_images / 255

test_images = array_reshape(test_images, c(10000, 28 * 28))
test_images = test_images / 255
```

### 목록 2.5 레이블 준비하기

```{r}
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
```

### 모델 훈련데이터 적합

```{r}
network %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
```

```{r}
metrics = network %>% evaluate(test_images, test_labels)
metrics
```

```{r}
network %>% predict(test_images[1:10,]) %>% k_argmax()
```

## 2.2 신경망에 대한 데이터 표현

텐서 : 임의의 차원 수로 이루어진 벡터 및 행렬의 일반화 개념

R에서 벡터는 1D 텐서를 만들고 조작하는 데 사용되며, 행렬은 2D 텐서에 사용된다.

더 상위인 차원의 경우, 임의의 수의 차원을 지원하는 배열 객체가 사용된다. 

### 2.2.1 스칼라

숫자가 한 개뿐인 텐서를 스칼라(scalar), 스칼라 텐서(scalar tensor), 0차원 텐서(zero-dimensional tensor) 또는 0D 텐서(0D 텐서)라고 한다. R에는 스칼라를 나타내는 데이터 유형이 없다(모든 숫자 객체는 벡터, 행렬 또는 배열임.) 그러나 항상 길이가 1인 R 벡터는 개념적으로 스칼라와 비슷하다.

### 2.2.2 벡터

1차원 배열로 구성된 수를 벡터(vector) 또는 1D 텐서(1D 텐서)라고 부른다. 1D 텐서는 정확히 하나의 축을 갖고 있다. R벡터를 배열 객체로 변환하면 차원을 확인해 볼 수 있다.

```{r}
x = c(12, 3, 6, 14, 10)
str(x)

dim(as.array(x))
```

### 2.2.3 행렬

숫자의 2차원 배열은 행렬(matrix), 즉 2D 텐서(2D tensor)이다. 행렬에는 행(raws)과 열(columns)이라 부르는 두 개 축이 있다. 행렬은 '직사각형 모양으로 된 숫자 격자'라는 식으로, 해석할 수 있다.

```{r}
x = matrix(rep(0, 3*5), nrow = 3, ncol = 5)
x

dim(x)
```

### 2.2.4 3D 텐서 및 고차원 텐서

새로운 행렬에 이러한 행렬들을 압축하면 시각적으로 정육면체라고 해석할 수 있는 3D 텐서를 얻을 수 있다.

```{r}
x = array(rep(0, 2*3*2), dim = c(2,3,2))
str(x)

dim(x)
```

배열에 3D 텐서를 꾸려 넣으면 4D 텐서를 생성할 수 있다. 딥러닝에서는 일반적으로 0D에서 4D까지의 텐서를 조작하지만, 비디오 데이터를 처리하는 경우, 5D까지 올라갈 수 있다.

### 2.2.5 주요 특성

텐서는 세 가지 주요 특성으로 정의된다.

- 축의 수(number of ranks) : 예를 들어, 3D 텐서에는 세 개축, 행렬에는 두 개 축이 있다.

- 모양(shapes) : 텐서가 각 축을 따라 몇 개 치수를 갖고 있는지를 나타내는 정수 벡터이다. 예를 들어, 앞의 행렬 예제는 (3, 5)모양이고, 3D 텐서 예제의 모양은 (2, 3, 2)이다. 벡터는 (5)와 같이 원소가 한 개뿐인 모양이다.

- 데이터 유형(data types) : 텐서에 포함된 데이터의 유형이다. 예를 들어, 텐서의 유형은 정수형(integer)이거나 배정도 실수형(double)일 수 있다. 드물기는 하지만, 문자형(character) 텐서를 볼 수도 있다. 그러나 텐서가 사전 할당된 인접 메모리 세그먼트에 있고, 가변 길이인 문자열은 이 구현의 사용을 배제하기 때문에 거의 사용되지 않는다.

#### 실제 MNIST 데이터셋의 예

```{r}
library(keras)
mnist = dataset_mnist()
train_images = mnist$train$x
train_labels = mnist$train$y
test_images = mnist$test$x
test_labels = mnist$test$y
```

```{r}
length(dim(train_images))

dim(train_images)

typeof(train_images)
```

```{r}
digit = train_images[5, , ]
plot(as.raster(digit, max = 255))
```

### 2.2.6 R에서 텐서 다루기

텐서 슬라이싱(tensor slicing) : 텐서에서 특정 원소를 선택하는 일

#### 10번에서 99번까지의 숫자 이미지를 선택하는 예

```{r}
my_slice = train_images[10:99, , ]
dim(my_slice)
```
```{r}
my_slice = train_images[10:99, 1:28, 1:28]
dim(my_slice)
```

#### 모든 이미지의 오른쪽 하단에 $14 \times 14$ 픽셀 선택의 예

```{r}
my_slice = train_images[, 15:28, 15:28]
dim(my_slice)
```

### 2.2.7 데이터 배치라는 개념

- 표본 축(sample axis) 또는 표본 차원(sample dimension) : 데이터 텐서의 첫 번째 축

- 딥러닝 모델은 학습 시에 전체 데이터셋을 한 번에 처리하기 보다, 작은 배치(batches)로 나눠서 처리한다.

- 배치 축(batch axis) 또는 배치 차원(batch dimension) : 배치 텐서를 고려했을 때의 첫 번째 축

```{r}
batch_1 = train_images[1:128, , ]
batch_2 = train_images[129:256, , ]

dim(batch_1)
dim(batch_2)
```

### 2.2.8 데이터 텐서의 실제 사례

- 벡터 데이터 : (표본, 특징) 모양으로 된 2D 텐서

- 일기 예보 데이터 또는 시퀀스 데이터 : (표본, 시간대, 특징) 모양으로 된 3D 텐서

- 이미지 : (표본, 높이, 너비, 채널) 모양 또는 (표본, 채널, 높이, 너비) 모양으로 된 4D 텐서

- 비디오 : (표본, 프레임, 높이, 너비, 채널) 모양 또는 (표본, 프레임, 채널, 높이, 너비) 모양으로 된 5D 텐서

### 2.2.9 벡터 데이터

가장 일반적인 데이터 텐서 종류로, 각 단일 데이터 점은 벡터로 부호화될 수 있으므로 데이터 배치는 2D 텐서(즉, 벡터 배열)로 부호화한다. 여기서 첫 번째 축은 표본 축(sample axis), 두 번째 축은 특징 축(feature axis)이다.

- 보험 통계 데이터셋 : 각 개인의 나이, 우편번호, 소득을 고려한 데이터셋. 각 사람은 세 가지 값의 벡터로 특징지을 수 있으므로 10만 명에 이르는 전체 데이터셋의 경우 (100000, 3) 모양으로 된 2D 텐서로 저장할 수 있다.

- 텍스트 문서 데이터셋 : 각 문서가 단어가 몇 번이나 등장하는지를 나타내는 데이터셋. 각 문서는 2만 개 값(사전에 들어 있는 한 개 단어당 한 개 값. 즉, 문서에는 그 단어의 출현 빈도수가 저장되어 있다.)으로 이뤄진 벡터로 부호화 될 수 있으므로 500개 문서로 된 전체 데이터 집합을 (500, 20000) 모양으로 된 텐서로 저장할 수 있다.

### 2.2.10 시계열 데이터 또는 시퀀스 데이터

데이터(또는 수열 순서의 개념)에서 시간이 중요할 때마다 명시적인 시간 축을 사용해 3D 텐서에 저장하는 게 바람직하다. 각 표본을 일련의 벡터(2D 텐서)로 부호화할 수 있으므로 데이터 배치는 3D 텐서로 부호화된다.

관례에 따라 시간 축은 항상 두 번째 축이다. 몇 가지 예를 살펴보자.

- 주가 데이터 셋 : 매분 주가의 현재 가격, 지난 1분간의 최고가 및 최저가로 저장된 데이터셋. 따라서 1분마다 크기가 3인 벡터가 되므로, 하루 동안 이뤄진 전체 거래는 (390, 3) 모양으로 된 2D 텐서(거래일에는 390분)으로 부호화되며, 250일분의 데이터를 (250, 390, 3) 모양으로 된 3D 텐서에 저장할 수 있다. 즉 여기에서의 각 표본은 하루 분량의 데이터가 된다.

- 트윗 데이터 셋 : 140개 크기를 가진 하나의 트윗에 대해, 128개 고유 영문자중 하나에 해당하는 경우 1, 그렇지 않으면 0을 부여하는 방식으로 크기가 128인 이진 벡터로 부호화할 수 있다. (one-hot encoding) 즉, 하나의 트윗에 대해 (140, 128) 크기의 2D 텐서로 부호화할 수 있으며, 100만 트윗의 데이터셋을 텐서 모양(1000000, 140, 128) 으로 저장할 수 있다.

### 2.2.11

일반적으로 이미지에는 높이(height), 너비(width) 및 색상 심도(color depth, 즉 색 깊이 또는 색심도)라는 세 가지 차원이 있다. 회색 음영 이미지(MNIST 숫자의 예)의 색상 채널은 한 개 뿐이므로 2D 텐서로 저장할 수 있지만, 이미지 텐서는 항상 3D이며, 회색조 (gray scale) 이미지에는 1차원 색상 채널이 있다. $256 \times 256$ 크기의 128개 회색조 이미지가 모인 배치 한 개는 (128, 256, 256, 1) 모양으로 된 텐서, 128개 색상 이미지로 이뤄진 배치 한 개는 (128, 256, 256, 3) 모양으로 된 텐서로 저장할 수 있다.

### 2.2.12

비디오 데이터는 5D 텐서가 필요한 몇 가지 유형의 실제 데이터 중 하나이다. 비디오는 프레임이 연달아 있는 형태로 이해할 수 있으며, 각 프레임은 컬러 이미지이다. 각 프레임은 3D 텐서(높이, 너비, 색상 심도)에 저장될 수 있고, 연속된 프레임은 4D 텐서 (프레임, 높이, 너비, 색상 심도)에 저장할 수 있다. 따라서 서로 다른 비디오들로 구성된 배치를 5D 텐서 모양(표본, 프레임, 높이, 폭, 색상 심도)에 저장할 수 있다.

## 2.3 신경망의 장비 : 텐서 연산

모든 컴퓨터 프로그램은 궁극적으로 이항 입력에 대한 작은 이항 연산(AND, OR, NOR 등)들로 구성된 집합으로 축소될 수 있으므로 심층 신경망에서 학습한 모든 변환도 텐서에 적용되는 소수의 수치 데이터에 관한 텐서 연산으로 줄일 수 있다. 예를 들어, 텐서를 더하거나 텐서를 곱하는 등의 작업을 할 수 있다.

```{r, eval = FALSE}
layer_dense(units = 512, activation = "relu")
```

이 계층은 2D 텐서를 입력으로 받아 다른 2D 텐서를 반환하는 함수로 해석될 수 있다. 구체적으로 함수는 다음과 같다.

```{r, eval = FALSE}
output = relu(dot(W, input) + b)
```

이 함수를 풀이해보면, input이라는 이름을 지닌 텐서와 $\mathbf{W}$라는 이름을 지닌 텐서사이의 내적(dot) 그리고 2D 텐서와 벡터 $b$ 사이의 덧셈(+), 마지막으로 렐루(ReLU)연산이라는 세 가지 텐서 연산이 있다. $\text{relu}(\mathbf{x}) = \text{max}(\mathbf{x}, 0)$이다.

### 2.3.1 원소별 연산

렐루 연산과 덧셈은 원소별(element-wise)연산이다. 즉, 텐서의 각 성분(entry, 즉 원소)에 독립적으로 적용되는 연산이다. 그러므로 이러한 작업은 대량 병렬 구현(벡터화된 구현, 1970~1990년의 벡터 프로세서를 사용하던 슈퍼컴퓨터 아키텍처에서 나온 용어)에 매우 적합하다.

원소 단위 연산을 단순하게 R로 구현하려면, 원소 단위 렐루 연산의 단순한 구현과 같이 `for` 루프를 사용해야 한다.
```{r}
# Generate a 4x4 matrix with random values from a standard normal distribution
x_mvn = matrix(rnorm(5 * 5), nrow = 5, ncol = 5)
y_mvn = matrix(rnorm(5 * 5), nrow = 5, ncol = 5)
print(x_mvn)
```

```{r}
naive_relu = function(x){
  for (i in 1:nrow(x)){
    for (j in 1:ncol(x)){
      x[i,j] = max(x[i,j], 0)
    }
  }
  return(x)
}

x_relu = naive_relu(x_mvn)
print(x_mvn)
print(x_relu)
```

덧셈도 같은 방식으로 처리한다.

```{r}
naive_add = function(x,y){
  for (i in 1:nrow(x)){
    for (j in 1:ncol(x)){
      x[i,j] = x[i,j] + y[i,j]
    }
  }
  return(x)
}

x_add = naive_add(x_mvn, y_mvn)
print(x_add)
```

같은 원리로 원소 단위로 곱셈과 뺄셈 등을 할 수 있다.

실제로는 R 배열들을 사용해 다룰 때, 최적화된 내장형 R 함수로 사용할 수 있는데, 이러한 내장 함수들은 무거운 작업을 BLAS 구현(Basic Linear Algebra Subprograms, 기본 선형 대수 서브프로그램)에 위임한다(따라서 BLAS를 설치해 둬야 한다). BLAS는 포트란 또는 C에서 일반적으로 구현되는 효율적인 저수준 병렬 처리 방식 텐서 조작 루틴이다.

R에서는 다음과 같이 원래의 원소별 연산들을 추종할 수 있으며, 연산들은 빠르게 처리될 것이다.

```{r}
z = x_mvn + y_mvn # 원소별 덧셈
print(z)
z = pmax(z, 0 ) # 원소별 렐루
print(z)
```

### 2.3.2 차원이 서로 다른 텐서와 관련된 연산

텐서의 덧셈시에, 두 개 텐서의 모양(shape)이 다른 경우, R의 `sweep()` 함수를 사용해 볼 수 있다.

R의 `sweep()`함수를 사용하면 더 높은 차원의 텐서와 낮은 차원의 텐서 간에 연산을 수행할 수 있다. `sweep()`을 사용하면 행렬에 벡터를 더하는 덧셈을 다음과 같이 수행할 수 있다.

```{r}
set.seed(42)
y_vec = matrix(rnorm(5), nrow = 5, ncol = 1)
# y_vec = as.vector(y_vec)
print(x_mvn)
print(y_vec)
sweep_ex = sweep(x_mvn, 1, y_vec, '+') # 여기서 2번째 인수는 y_vec을 스윕시킬 x_mvn의 차원을 지정한다.
print(sweep_ex)
```

여기서 2번째 인수는 y_vec을 스윕시킬 x_mvn의 차원을 지정한다. 마지막 인수(여기서는 +)는 스윕하는 중에 수행할 연산이며, 두 개 인수로 이루어진 함수여야 한다. 2번째 인수가 1이면 행에 대해서 y_vec을 주르륵 브로드캐스팅하여 덧셈, 2이면 열에 대해서 y_vec을 주르륵 브로드캐스팅하여 덧셈.

몇 개 차원이든 스윕 적용가능하며, 두 배열을 통해 벡터화된 연산을 구현하는 함수를 적용할 수 있다. 다음 예제는 `pmax()` 함수를 사용해 4D 텐서의 마지막 두 차원을 대상으로 2D 텐서를 스윕한다.

```{r}
x = array(round(runif(64* 3* 32* 10, 0, 9)), dim = c(64, 3, 32, 10))
y = array(5, dim = c(32, 10))

z = sweep(x, c(3,4), y, pmax)

print(dim(z))
# print(z[20, 1, , ])
```

### 2.3.3 텐서 내적

내적 연산은 텐서 곱(tensor product)이라고도 하며, (원소별 곱과 다른 개념이다)가장 일반적이고, 유용한 텐서 연산이다. 원소별 연산과 달리 입력 텐서의 성분을 결합한다.

R에서는 원소별 곱을 * 연산자로 처리하는 반면, 내적(dot products, 점곱)에는 %*% 연산자를 사용한다.

```{r}
x_vec = runif(10, -9, 9) ; y_vec = runif(10, -9, 9)
print(x_vec)
print(y_vec)
z = x_vec %*% y_vec
print(z)
```

```{r}
naive_vector_dot = function(x, y){
  z = 0
  for (i in 1:length(x)){
    z = z + x[[i]] * y[[i]] 
    # x[2] 는 x = (2,3,1,4,5)의 경우 (3)을 반환한다. 그러나 x[[2]]는 3을 반환한다. 즉 [[]] 연산자는 원소 그자체를 꺼낸다.
  }
  return(z)
}
z = naive_vector_dot(x_vec, y_vec)
print(z)
```

두 벡터 사이의 내적(dot product, 즉 스칼라 곱)은 스칼라이고, 원소 개수가 서로 같은 벡터만 내적과 호환된다는 것을 알 수 있다.

행렬 $\mathbf{x}$와 벡터 $\mathbf{y}$ 사이의 내적을 취할 수도 있다. 벡터는 $\mathbf{y}$와 $\mathbf{x}$ 의 행들 사이의 내적이 되는 벡터를 반환한다. 다음과 같이 구현할 수 있다.

```{r}
naive_matrix_dot = function(x, y){
  z = rep(0, nrow(x))
  for (i in 1:nrow(x)){
    for (j in 1:ncol(x)){
      z[[i]] = z[[i]] + x[[i,j]] * y[[j]]
    }
  }
  return(z)
}
set.seed(42)
x_mat = array(runif(40, -9, 9), dim = c(4,10))
z = naive_matrix_dot(x_mat, y_vec)
print(z)
print(typeof(z))
print(class(z))
```

```{r}
naive_matrix_dot = function(x, y){
  z = rep(0, nrow(x))
  for (i in 1:nrow(x)){
    z[[i]] = naive_vector_dot(x[i,], y)
  }
  return(z)
}
set.seed(42)
x_mat = array(runif(40, -9, 9), dim = c(4,10))
z = naive_matrix_dot(x_mat, y_vec)
print(z)
```

내적은 임의의 수의 축을 가진 텐서로 일반화되며, 이 경우 가장 일반적인 애플리케이션은 두 행렬 사이의 내적이라고 할 수 있다.

```{r}
naive_matrix_dot = function(x,y){
  z = matrix(0, nrow = nrow(x), ncol = ncol(y))
  for (i in 1:nrow(x)){
    for (j in 1:ncol(y)){
      row_x = x[i, ] ; col_y = y[,j]
      z[i, j] = naive_vector_dot(row_x, col_y)
    }
  }
  return(z)
}

set.seed(42)
x_mat = array(runif(20, -9, 9), dim = c(4,5))
y_mat = array(runif(10, -9, 9), dim = c(5,2))
z = naive_matrix_dot(x_mat, y_mat)
z_r = x_mat %*% y_mat
print(z == z_r)
```

더 일반적으로 2D 사례와 관련해서는 설명한 대로 모양 호환성에 대한 동일한 규칙에 따라 더 높은 차원의 텐서 간 내적을 가져올 수 있다.

```{r, eval = FALSE}
(a, b, c, d) . (d) -> (a, b, c)
(a, b, c, d) . (d, e) -> (a, b, c, e)
```

### 2.3.4 텐서 모양 변경

세 번째 유형의 텐서 연산은 텐서 모양 변경(tensor reshaping)이다. 텐서 모양 변경을 첫 번째 신경망 예제에 나오는 조밀 계층에 사용하지는 않았지만, 망에 입력하기 전, 숫자 데이터를 전처리할 때는 사용했었다.

```{r, eval = FALSE}
train_images = array_reshape(train_images, c(60000, 28 * 28))
```

`dim -< ()` 함수보다는 `array_reshape()` 함수를 사용해 배열의 모양을 변경한다는 점에 주목하자. 이는 행-중심 구문(row-major semantics)을 사용해 데이터가 재해석되도록 하기 위한 것으로 (R의 기본인 열 중심 구문과 반대), 한편으로 케라스가 호출한 수치 처리 라이브러리(NumPy 및 텐서플로 등)가 배열 차원을 해석하는 방식과 호환된다. 케라스에 전달될 R 배열을 다시 만들 때는 항상 `array_reshape()`함수를 사용해야 한다.

텐서 모양 변경이란, 행과 열을 표적의 모양과 일치하도록 재조정하는 것을 말한다. 당연히, 모양이 변경된 텐서에 사용하는 계수들의 총 개수는 초기 텐서와 동일해야 한다. 다음과 같은 예제를 통해 이러한 모양 변경을 가장 잘 이해할 수 있다.

```{r}
x = matrix(c(0, 1, 2, 3, 4, 5), nrow = 3, ncol = 2, byrow = TRUE)
x

x = array_reshape(x, dim = c(6,1))
x

x = array_reshape(x, dim = c(2,3))
x

```

흔히 발생하는 모양 변경의 특별한 경우는 전치(transpose)이다. 행렬의 전치는 `x[i, ]`가 `x[ ,i]`로 되도록 행과 열을 교환하는 것을 의미한다. 행렬을 전치하는 데는 `t()`함수를 사용할 수 있다.

```{r}
x = matrix(0, nrow = 300, ncol = 20)
dim(x)

x = t(x)
dim(x)
```

### 2.3.5 텐서 연산의 기하학적 해석

텐서 연산에 의해 조작된 텐서들의 내용은 어떤 기하 공간에서 점들의 좌표로 해석될 수 있기 때문에 모든 텐서 연산에는 기하학적 해석이 따른다.

일반적으로 어파인 변환, 회전, 척도 구성등과 같은 기본 기하 연산은 텐서 연산으로 표현될 수 있다. 예를 들어, 각도 $\theta$에 의한 2D 벡터의 회전은 $2 \times 2$행렬`R = [u,v]` 를 갖는 내적을 통해 이뤄질 수 있다. 여기서 `u`와 `v`는 평면의 벡터가 된다. 즉, `u = [cos(theta), sin(theta)]`, `v = [-sin(theta), cos(theta)]` 이다.

### 2.3.6 딥러닝의 기하학적 해석

신경망이 전적으로 텐서 연산의 연쇄(chains of tensor operations)로 구성되어 있고, 이러한 모든 텐서 연산이 입력 데이터의 기하학적 변환이라는 것을 알았다. 따라서 일련의 간단한 단계를 통해 구현된 고차원 공간에서 신경망을 매우 복잡한 기하학적 변환으로 해석할 수 있다.

(종이 공 예시).. 즉, 종이 공 펴기란, 복잡할 정도로 많이 접힌 데이터 다양하체(data manifolds)에 대한 깔끔한 표현을 찾는 머신러닝에 관한 것이다. 지금쯤이면 여러분은 딥러닝이 탁월한 이유를 잘 직감할 수 있어야 한다. 복잡한 기하학적 변환을 점진적으로 분해하여 기초 구성 요소들의 긴 연쇄로 만드는 접근 방식이 필요하다는 점 말이다. 이는 종이 공을 펼칠 때 인간이 따라야 할 전략과 거의 같다. 심층 망의 각 계층은 데이터를 약간 풀어 펴는 변형을 하는 역할을 하는 셈이 되고, 깊게 겹쳐 쌓인 각 계층들이 모여서 극도로 복잡한 펴기 과정을 다루기 쉽게 한다.

즉, 종이 공이 접힌 각도나 횟수, 모양에 맞게 신경망 모형의 계층을 구성한다면 적당한 양의 데이터가 주어졌을 때, 해당 종이 공을 펴지 않고도 무슨 종이로 구성되어있는지 매우 정확하게 맞출 수 있을 것이다. 반대로 계층을 적당하게 구성하지 못한다면 (계층수가 더 많거나 혹은 더 적거나, 계층안의 퍼셉트론수가 적거나 많거나 등등) 앞서의 신경망 모형의 경우보다 학습에 필요한 데이터양이 더 늘어날 수도 있으며, 혹은 학습 데이터양을 아무리 많이 늘리더라도 학습이 잘 되지 않을 수도 있다.




