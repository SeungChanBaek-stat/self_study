---
title: "고급회귀분석 15장"
output: html_document
date: "2025-04-30"

header-includes:
  - \newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 15.1 다중공선성의 문제

```{r}
source("functions/mult_reg.R", echo = F)
source("functions/reg_diagnostics.R", echo = F)
source("functions/var_selection.R", echo = F)
source("functions/anova_reg.R", echo = F)
```

```{r}
x1 = rnorm(10, mean = 0, sd = 1)
x2 = rnorm(10, mean = 1, sd = 1)
x3 = rnorm(10, mean = 2, sd = 1)

X = cbind(x1, x2, x3)

vif_vec = vif_detect(X)

vif_vec
```

## 15.6 R 실습

이 절에서는 11장 5절 R 실습에서 소개한 자동차 연비(gas mileage) 데이터를 가지고 이 장에서 다룬 편의 추정량에 대해 알아보기로 한다. 해당 절에서 살펴보았지만, 설명변수 `cyl(엔진의 기통수)`, `disp(배기량)`, 그리고 `disp(배기량)`과 `wt(무게)` 등은 높은 상관관계를 나타내었다. 따라서, 이들 설명변수를 모두 사용하는 경우 다중공선성의 문제가 나타난다. 이를 확인하기 위해 분산팽창계수(VIF)를 구해보도록 한다. 이것은 `car`패키지에서 `vif()`함수를 통해 다음과 같이 구할 수 있다.

```{r}
library(car)
library(dplyr)
fit <- lm(mpg ~ ., data = mtcars)
round(vif(fit), 2)

## vif_vec함수와의 비교 : car 패키지안에 같은 이름인 select()함수가 들어있어서 dplyr패키지를 명시적으로 실행시켜줘야한다.
X = mtcars %>%
  dplyr::select(cyl:carb)
vif_vec = vif_detect(X)
round(vif_vec, 2)
```

위 결과에서 보면 위에서 언급한 `cyl`, `disp`, `wt`가 10이상의 VIF를 갖게 되어 다중공선성이 있음을 확인할 수 있다. 이에 최소제곱추정량 대신 편의 추정량을 구하도록 한다. 단, 본문에서는 VIF를 설명하면서 표준화된 변수를 가정했는데, 변수의 표준화 여부에 상관없이 VIF는 같은 값을 갖는다. (이유 : 연습문제 8.8 - 선형회귀모형 $\mathbf{y} = X\boldsymbol{\mathbf{\beta}} + \boldsymbol{\epsilon}$ 에서 결정계수 $R^{2}$은 $y_i$나 $x_ij$의 단위가 바뀌어도 관계없이 항상 일정하기 때문이다.)

### 15.6.1 능형회귀

먼저 능형회귀모형을 적합해 보자. 능형회귀모형은 `MASS`패키지에 있는 `lm.ridge()`함수를 통해 다음과 같이 적합할 수 있다. 먼저 설명변수와 반응변수 모두 `scale()`함수를 통해 표준화를 한 후, 회귀모형을 적합하기로 한다. `R`에서는 조절 모수 $k$를 $\lambda$로 표현하는데, 이 예제에서는 $\lambda$를 0.01부터 100까지 0.01 간격으로 변화시키며 능형추정량을 적합하고 있다.

```{r}
library(MASS)
sc.mtcars <- data.frame(scale(mtcars))
rfit <- lm.ridge(mpg ~ ., data = mtcars, lambda = seq(0.01, 100, 0.01))
matplot(rfit$lambda, t(rfit$coef), type = "l",
        xlab = expression(k), ylab = expression(bold(beta)(k)))
```


```{r}
## ridge_reg와의 비교 : 표준화를 어떻게 하느냐에 따라 결과가 달라진다
library(dplyr)
source("functions/biased_reg.R", echo = F)
X = mtcars %>%
  dplyr::select(cyl:carb)
y = mtcars %>%
  dplyr::select(mpg)
lambda = seq(0.01, 100, 0.01)

ridge_coef = ridge_reg(X = X, y = y, lambda = lambda)
matplot(lambda, t(ridge_coef), type = "l",
        xlab = expression(k), ylab = expression(bold(beta)(k)))
```

위의 플롯은 `matplot()`함수를 통하여 능형추정값 $\hat{\beta}_j(k)$에 대하여 능형트레이스를 그린 것이다. 설명변수가 모두 10개 이므로 10개의 곡선이 그려진다. 능형추정량에서 적절한 $k$를 선택하는 문제는 19장 교차검증에서 자세히 소개하기로 한다.

$k=0$인 경우 능형회귀추정량이 최소제곱추정량과 같음을 다음과 같이 R에서 확인해볼 수도 있다.

```{r}
ridge_fit = lm.ridge(mpg ~ ., data = sc.mtcars, lambda = 0)
lse_fit = lm(mpg ~ . , data = sc.mtcars)

round(ridge_fit$coef, 3)
round(lse_fit$coef[-1], 3)
```

또한 $k$가 작을 때와 클 때 추정량의 성질을 비교해 보기 위해 $k = 14.91$을 적합한 결과를 살펴보면 다음과 같다.

```{r}
round(rfit$coef[,rfit$lambda == "14.91"], 3)
```

위 결과에서 알 수 있듯이 $k = 0$인 경우에 비해 $k$가 큰 경우 회귀계수가 작아지는 경향이 있다.

## 15.6.2 주성분회귀

이제 주성분회귀모형을 적합하기 위해 주성분을 구하기로 한다. 자동차 연비 데이터의 경우 설명변수들의 측정단위가 다르기 때문에, 주성분을 찾기 위해 설명변수에 대한 표준화를 하기로 한다.(주성분의 개수를 결정하기 위해서는 설명변수를 표준화하여 모든 변수들이 같은 척도를 갖도록 해야 한다. 표준화를 하지 않는 경우 분산이 큰 변수들이 주성분 결정에 큰 역할을 하는 경향이 발생하는 등 그 결과가 변수의 단위에 민감하게 나타난다. 예를 들어 한 변수만 g으로 관측되고 다른 변수는 모두 kg으로 관측된 경우, 자료의 산포는 g으로 관측된 변수에 의존하게 된다.) 표준화 이후 주성분을 찾는 것은 상관계수 행렬을 이용하여 주성분 분석을 하는 것과 동일하다. `eigen()`함수의 결과로 고윳값 `e$values`와 고유벡터 `e$vectors`를 구할 수 있다. 고윳값들을 대각원소로 하는 대각행렬ㄹ이 $\Lambda$이고, 고유벡터들의 행렬은 $P$가 된다.

```{r}
x <- as.matrix(mtcars[,-1])
e <- eigen(cor(x))
e$values
```

위 결과로부터 $\mathbf{y} = Z \boldsymbol{\mathbf{\alpha}} + \boldsymbol{\epsilon}$의 모형을 적합해 보자. 주성분 $Z = XP$는 표준화된 설명변수 $X$의 선형결합으로 구해야 한다. 상관계수에 대하여 고윳값과 고유벡터를 구했기 때문에 먼저 `scale()`함수를 통해 설명변수를 표준화한 후 주성분을 구하여 회귀모형을 적합한다.

```{r}
s.X <- scale(X) # 설명변수의 표준화
Z <- s.X %*% e$vectors # Z = XP
pcr <- lm(mtcars$mpg ~ Z)
summary(pcr)
```

여기서 주성분회귀모형의 장점은 새로운 설명변수 $Z_1, \cdots, Z_{10}$들이 서로 직교한다는 것이다. 한편, 위 결과로부터 두 번째 주성분은 유의하지 않은 반면에 세 번째 주성분은 유의하게 나타나는 것을 주목할 필요가 있다. 두 번째 고유벡터 방향이 설명변수들이 두 번째로 많이 많이 몰려 있는 방향을 나타내긴 하지만, 이 결과를 통해 주성분 순서대로 반응변수와의 유의성을 보장하지 않음을 알 수 있다.

위 결과로부터 $\Lambda_g \hat{\boldsymbol{\mathbf{\alpha}}}_g = P^{\top}_g X^{\top}\mathbf{y}$ 에 있는 주성분의 개수 $g$를 결정하기 위해 다음과 같이 산비탈 그림(scree plot)을 그려보기로 한다.

```{r}
plot(e$values, type = "l", xlab = "No. of components")
```

위 산비탈 그림으로부터 고윳값이 크게 줄어들어 앞서 관측된 값에 비해 현저하게 값이 작아 무시할 수 있는 지점(elbow)을 확인할 수 있다. 위 자료에서는 주성분의 개수가 3부터 크게 줄어들어 3개의 성분을 취하는 것을 고려해 볼 수 있다. 지금까지의 계산은 $\boldsymbol{\mathbf{\alpha}}$ 에 대한 추정량이고 $\hat{\boldsymbol{\mathbf{\beta}}} = P \hat{\boldsymbol{\mathbf{\alpha}}}$ 으로부터 $\boldsymbol{\mathbf{\beta}}$에 대한 추정량을 구할 수 있다.

### 15.6.3 부분최소제곱회귀

이제 부분최소제곱회귀에 대하여 살펴보자. 먼저 모든 설명변수와 반응변수에 대하여 중심화(centering)를 통해 각각의 기대값을 0으로 한다.

```{r}
x <- as.matrix(mtcars[, -1])
y <- mtcars$mpg
cx <- sweep(x, 2, apply(x, 2, mean))
cy <- y - mean(y)
```

여기서 설명변수 행렬(`x`)의 각 열에 대한 표본평균과 반응변수(`y`)의 표본평균을 0으로 하기 위해 `cx`와 `cy`를 생성한다. 설명변수가 모두 10개이므로, 이들로부터 단순선형회귀모형을 적합한 10개의 회귀계수는 다음과 같이 계산할 수 있다.

```{r}
phi1 <- rep(0, 10)
for (j in 1 : length(phi1)){
  phi1[j] <- crossprod(cx[,j], cy) / crossprod(cx[,j], cx[,j])
}
round(phi1, 1)
```

```{r}
ncx <- sweep(cx, 2, phi1, "*")
t1 <- apply(ncx, 1, mean)
```

여기서 `ncx`는 `cx`에 있는 설명변수와 회귀계수 추정값과의 곱을 계산한 것으로 $V_{1j}$와 $\hat{\phi}_{1j}$의 곱으로 생각할 수 있다. 또한 위와 같이 정의된 `t1`은 $w_{1j} = 1/p = 1/10$을 사용하여 구한 결과이다. 이제 새롭게 생성된 설명변수 $T_1$(`t1`)이 반응변수 `y`를 얼마나 잘 예측하는지 살펴보도록 하자.

```{r}
pls1 <- lm(cy ~ t1 - 1)
summary(pls1)
```

위 결과를 $g = 1$인 주성분 회귀 결과와 비교해 보자.

```{r}
Z1 <- Z[,1] # 1st component
pcr1 <- lm(cy ~ Z1 - 1)
summary(pcr1)
```

결정계수 $R^2$ 을 비교해 보면 부분최소제곱으로 선택한 선형결합 $T_1$의 반응변수에 대한 설명력이 $Z_1$보다 약간 더 높은 것을 알 수 있다.

이제 $w_{1j}$가 설명변수 $x_j$의 분산에 비례하는 경우를 살펴보자.

```{r}
varx <- apply(cx, 2, var)
vncx <- sweep(ncx, 2, varx, "*")
t1a <- apply(vncx, 1, sum)/sum(varx)
pls1a <- lm(cy ~ t1a - 1)
summary(pls1a)
```

이 결과를 앞의 결과와 비교해보면 설명력($R^2$)이 약간 떨어지는 것으로 보인다.

이제 부분최소제곱회귀의 두 번째 성분을 계산해 보자. 이를 위해 각 설명변수와 반응변수 모두 첫 번째 성분 $T_1$에 대한 회귀모형에서의 잔차를 구해야 하고, 이들 값을 새로운 설명변수와 반응변수로 하여 위의 절차를 따르면 된다.

```{r}
cx2 <- matrix(0, 32, 10) # dim(x) = (32, 10)
for (j in 1: 10){
  cx2[,j] <- lm(cx[,j] ~ t1 - 1)$res
}
cy2 <- lm(cy ~ t1 - 1)$res

phi2 <- rep(0, 10)
for (j in 1:length(phi2)){
  phi2[j] <- crossprod(cx[,j], cy2)/crossprod(cx[,j], cx[,j])
}
ncx2 <- sweep(cx2, 2, phi2, "*")
t2 <- apply(ncx2, 1, mean)
```

이때 두 성분 $T_1$과 $T_2$의 상관계수를 살펴보면 거의 0과 같음을 알 수 있다.

```{r}
cor(t1, t2)
```

이제 두 성분 $T_1$과 $T_2$를 사용하여 회귀모형을 적합하면 다음과 같다.

```{r}
pls2 <- lm(cy ~ t1 + t2 - 1)
summary(pls2)
```

첫 번째 성분($T_1$)의 회귀계수 추정값이 첫 번째 성분만 사용한 부분최소제곱회귀에서의 회귀계수와 차이가 없는 것을 확인할 수 있다. 위 결과를 두 개의 주성분 $Z_1$과 $Z_2$를 사용한 주성분 회귀모형의 결과와 비교해 보자.

```{r}
pcr2 <- lm(cy ~ Z[,1:2] - 1)
summary(pcr2)
```

결과를 비교해 보면 주성분회귀에 비하여 부분최소제곱회귀가 반응변수에 대한 설명력($R^2$)이 높게 나타난다. 여기서 몇 개의 성분을 사용할 것인가 하는 것은 교차검증을 통하여 정할 수 있다. 이 부분과 관련해서는 19장에서 자세히 알아보기로 한다.

추가로 주성분회귀와 부분최소제곱회귀에 대해서는 `pls` 패키지의 `pcr()`함수와 `pls()`함수를 사용할 수 있으며, 이 함수 사용의 결과와 위에서 소개한 프로그램 간의 관계가 어떻게 일치하는지 살펴보자. 먼저 첫 번째 주성분 $Z_1$만 사용한 주성분회귀 결과와 `pls`패키지의 `pcr()`함수를 사용한 경우를 비교해 보자. `pcr()` 함수에서 `ncomp = 1`을 사용하여 첫 번째 주성분만 사용한 주성분회귀를 할 수 있다. 이 모형의 결과로부터 주성분과 위에서 고윳값 분해를 통해 구한 주성분을 비교해 보면 방향만 다른 것을 알 수 있다.

```{r}
# install.packages("pls")
library(pls)
pcr.fit <- pcr(cy ~ cx, scale = T, ncomp = 1)
cbind(pcr.fit$score, Z[,1]) # 주성분 비교
```

실제로 `pcr()`함수를 통해 적합한 값과 첫 번째 주성분($Z_1$)을 사용하여 단순선형회귀모형(`pcr1`)을 적합한 결과를 살펴보면 거의 같음을 알 수 있다.

```{r}
pc.yhat <- predict(pcr.fit, cx)
pc.yhat1 <- predict(pcr1, as.data.frame(cx))
cbind(pc.yhat, pc.yhat1)
```

이번에는 부분최소제곱회귀를 `plsr()`함수를 통해 적합한 결과와 첫 번째 주성분($T_1$)을 사용하여 단순선형회귀모형 (`pls1`)을 적합한 결과를 살펴보도록 하자.

```{r}
pls.fit <- plsr(cy ~ cx, scale = T, ncomp = 1)
yhat <- predict(pls.fit, cx)
yhat1 <- predict(pls1, as.data.frame(cx))
cbind(yhat, yhat1)
```

이들의 적합결과도 거의 일치함을 알 수 있다. 여기서 `pls.fit`결과를 잠깐 살펴보면, `plsr()`함수에서 부분최소제곱회귀에 대한 알고리즘으로 `kernelpls`가 디폴트로 쓰이는 것을 알 수 있는데, 다른 방법이 선택되어도 같은 결과를 주는 것을 확인할 수 있다.

```{r}
summary(pls.fit)
```

이 절에서 소개한 것은 부분최소제곱회귀의 기본이 되는 알고리즘으로 이보다 좀 더 빠르게 계산할 수 있는 다양한 알고리즘들이 개발되어 있으며, 이와 관련한 내용은 (참고문헌 15.10)을 참고한다.

## 연습문제

### 15.14

```{r}
source("functions/mult_reg.R", echo = F)
source("functions/biased_reg.R", echo = F)
library(ISLR)
library(dplyr)

Hitters <- na.omit(Hitters)
dim(Hitters)

Hitters

hitters_dat = Hitters %>%
  dplyr::select(AtBat:NewLeague)


# hitters_dat[,"Salary"]
X = hitters_dat %>%
  dplyr::select(-Salary)
y = hitters_dat %>%
  dplyr::select(Salary)
```

먼저 가변수열을 만들어보자.

```{r}
X_dummy = dummy_var_gen(X)

X_test = scale(X_dummy, center =TRUE, scale = TRUE)
```

#### 능형회귀

생성된 가변수열로 구성된 데이터로 능형회귀를 적합해보면 다음과 같다.

```{r}
lambda = seq(0.01, 100, 0.01)
ridge_res = ridge_reg(X = X_dummy, y = y, lambda = lambda)
```

능형추정값 $\hat{\beta}_j (k)$에 대하여 능형 트레이스를 그려 보면 다음과 같다.

```{r}
ridge_coef = ridge_res
matplot(lambda, t(ridge_coef), type = "l",
        xlab = expression(k), ylab = expression(bold(beta)(k)))
```

```{r}
# 1) 기존 파라미터 백업
op <- par(no.readonly = TRUE)

# 2) 마진 조정 (아래, 왼쪽, 위, 오른쪽)
#    기본값은 c(5, 4, 4, 2) + 0.1 인데, 
#    오른쪽을 넉넉히 8로 늘려줌
par(mar = c(5, 4, 4, 8) + 0.1)

# 3) matplot 그리기
matplot(lambda,
        t(ridge_coef),
        type = "l",
        lty  = 1,
        col  = 1:ncol(ridge_coef),
        xlab = expression(k),
        ylab = expression(bold(beta)(k)),
        main = "야구선수 데이터의 능형트레이스")

# 4) xpd=NA 로 클리핑 해제하고, legend 외부로 배치
par(xpd = NA)
legend("topright",
       inset  = c(-0.25, 0),            # 오른쪽 바깥쪽으로 20% 뺌
       legend = rownames(ridge_coef),
       col    = 1:ncol(ridge_coef),
       lty    = 1,
       cex    = 0.6,
       bg     = "white")

# 5) 파라미터 원복
par(op)
```

플롯에서 알 수 있듯이, 야구선수데이터의 설명변수가 모두 19개이므로 19개의 곡선이 그려진다. 능형추정량에서 $k = 0$ 인 경우 능형회귀추정량이 최소제곱추정량과 같음을 다음과 같이 확인할 수 있다.

```{r}
## 표준화
X_dummy_scaled <- data.frame(scale(X_dummy))
y_scaled <- data.frame(scale(y))
X_mat = as.matrix(X_dummy_scaled) ; y_mat = as.matrix(y_scaled)

print(glue("\n"))
## 최소제곱추정법
lse_res = mult_reg(X_mat, y_mat, alpha = 0.05, coeff = TRUE)
print(t(round(lse_res$beta_hat, 2)))

## k = 0 능형회귀
ridge_k0_res = ridge_reg(X_mat, y_mat, lambda = c(0))
print(t(round(ridge_k0_res, 2)))
```

또한 $k$ 가 큰 경우 회귀계수가 작아지는 경향을 다음과 같이 확인해볼 수 있다.

```{r}
round(ridge_coef[,lambda == "14.91"], 2)
```
기존 라이브러리를 활용한 코드는 다음과 같다.

```{r}
X_dummy_scaled <- data.frame(scale(X_dummy))
y_scaled <- data.frame(scale(y))

hitters_scaled <- cbind(X_dummy_scaled, y_scaled)
rfit_scaled <- lm.ridge(Salary ~ ., data = hitters_scaled, lambda = seq(0.01, 100, 0.01))
matplot(rfit_scaled$lambda, t(rfit_scaled$coef), type = "l",
        xlab = expression(k), ylab = expression(bold(beta)(k)))
```

#### 주성분회귀

주성분회귀도 능형회귀와 마찬가지로 설명변수에 대한 표준화가 필요하므로 이전에 사용한 `X_mat`, `y_mat`을 그대로 사용하기로 한다.

```{r}
pc_res = pc_reg(X_mat, y_mat, alpha = 0.05)

R2 = pc_res$SSR / pc_res$SST ; R2
```

야구선수 데이터의 경우 산비탈 그림(scree plot)만으로 주성분의 개수 $g$를 결정하기는 어려워 보인다. $g= 3, 4, 5$ 모두 앞서 관측된 값에 비해 고윳값이 계속해서 작아지는데, $k= 8, 9$에서도 유의미한 수준으로 고윳값이 작아지기 때문에 elbow point를 정하기 애매해 보인다. 굳이 고르자면 $g = 5$일때가 적절해보이기는 하나 이것이 올바른 결정이라고 확신하기에는 어렵다.

다음은 `lm()` 함수를 활용해서 각 주성분의 유의성을 확인하는 코드이다.

```{r}
eigen_decomp = eigen(cor(X_mat))
Z = X_mat %*% eigen_decomp$vectors
pcr_res = lm(y_mat ~ Z)
summary(pcr_res)
```

앞서 산비탈그림에서 확인했듯이, 19개의 설명변수중 몇개의 새로운 설명변수만이 유의함을 확인할 수 있다. 또한 세 번째 설명변수 $Z_3$은 $Z_{14}, Z_{16}$보다 고윳값이 더 큰 경우에 해당되나, 반응변수 $y$와의 유의성은 오히려 이들보다 더 떨어짐을 알 수 있다. 이 결과를 통해 주성분 순서대로 반응변수와의 유의성이 보장되지 않음을 알 수 있다. 그리고 결정계수 값이 0.5461로, 주성분회귀모형은 반응변수를 제대로 설명하지 못한다는 점도 확인할 수 있다.

#### 부분최소제곱회귀

```{r}
source("functions/biased_reg.R", echo = F)
X = hitters_dat %>%
  dplyr::select(-Salary)
y = hitters_dat %>%
  dplyr::select(Salary)

X_dummy = dummy_var_gen(X)

X_mat <- as.matrix(X_dummy)
y_mat <- as.matrix(y)

## 부분최소제곱회귀 - 방법 1 : 직교변환 불변 방법
pls_res1 = pls_reg(X_mat, y_mat, alpha = 0.05, method = "ortho_invar")
```

```{r}
cor(pls_res1$T_mat)
```

보다시피 가중치를 $X$의 직교변환에 대하여 반응변수 $y$의 예측이 불변하게끔 설정한 경우 $l=3$까지는 결정계수값이 크게 증가하다가 이후부터는 증가폭이 줄어드는 것을 확인할 수 있다. 또한 생성된 $T = [\mathbf{t}_1, \ldots, \mathbf{t}_{19}]$ 는 거의 단위행렬에 가까운 직교행렬로, $X$에 대한 연관성을 고려함과 동시에 반응변수 $y$와의 연관성까지 고려하는 행렬이다.

```{r}
## 부분최소제곱회귀 - 방법 2 : 척도변환 불변 방법
# pls_res2 = pls_reg(X_mat, y_mat, alpha = 0.05, method = "scale_invar")
```

가중치를 $X$의 척도변환에 대하여 반응변수 $y$의 예측이 불변하게끔 설정한 경우에는 $l = 3$에서 PLS 알고리즘 계산이 singular value 문제로 인해 멈추는데, 이는 곧 야구선수 데이터에 대해서 첫 3개의 PLS 잠재변수들만이 핵심적인 정보를 담고 있다고 해석할 수 있다. 예를 들어 계산되는 $T_4$의 경우에는 앞에서 계산된 $T_1, T_2, T_3$와 거의 선형종속에 가까울 정도로 연관성이 높기 때문에 $T_4$ 계산은 의미가 없게 되는 것이다. 
