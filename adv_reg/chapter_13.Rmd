---
title: "고급회귀분석 13장"
output: html_document
date: "2025-04-10"

header-includes:
  - \newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 13.1 설명변수로 한 개의 가변수를 갖는 경우

```{r}
source("functions/mult_reg.R", echo = TRUE)
source("functions/reg_diagnostics.R", echo = TRUE)
source("functions/var_selection.R", echo = TRUE)
```

```{r}
x1 = c(151, 92, 175, 31, 104, 277, 210, 120, 290, 238,
       164, 272, 295, 68, 85, 224, 166, 305, 124, 246)
x2 = c(rep(0, 10), rep(1, 10))
y = c(17, 26, 21, 30, 22, 1, 12, 19, 4, 16,
      28, 15, 11, 38, 31, 21, 20, 13, 30, 14)

X = cbind(x1, x2)

res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
res$beta_hat
```

```{r}
# X[1:10,]
# y_hat_0 = X[1:10,] %*% res$beta_hat
# y_hat_1 = X[11:,] %*% res$beta_hat

plot(x = x1[1:10], y = y[1:10], xlab = "적성검사점수", ylab = "총 소요시간",
     main = "성별에 따른 회귀모형", col = "black", bg = "black", pch = 21, cex = 1, xlim = c(0, 320), ylim = c(0, 40))
points(x = x1[11:20], y = y[11:20], col = "black", pch = 2, cex = 1)
abline(coef = c(res$beta_hat[1] + res$beta_hat[3], res$beta_hat[2]), col = "lightblue", lwd = 2)
abline(coef = c(res$beta_hat[1], res$beta_hat[2]), col = "blue", lwd = 2)
```

```{r}
x1 = c(151, 92, 175, 31, 104, 277, 210, 120, 290, 238,
       164, 272, 295, 68, 85, 224, 166, 305, 124, 246)
x2 = c(rep(0, 10), rep(1, 10))
y = c(17, 26, 21, 30, 22, 1, 12, 19, 4, 16,
      28, 15, 11, 38, 31, 21, 20, 13, 30, 14)
x1_x2 = x1 * x2
X = cbind(x1, x2, x1_x2) ; X

res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
res$beta_hat

plot(x = x1[1:10], y = y[1:10], xlab = "적성검사점수", ylab = "총 소요시간",
     main = "성별에 따른 회귀모형", col = "black", bg = "black", pch = 21, cex = 1, xlim = c(0, 320), ylim = c(0, 40))
points(x = x1[11:20], y = y[11:20], col = "black", pch = 2, cex = 1)
abline(coef = c((res$beta_hat[1] + res$beta_hat[3]), (res$beta_hat[2] + res$beta_hat[4])), col = "lightblue", lwd = 2)
abline(coef = c(res$beta_hat[1], res$beta_hat[2]), col = "blue", lwd = 2)

res$beta_hat[2] + res$beta_hat[4]
res$beta_hat[2]
```

```{r}
beta3_hat = res$beta_hat[4]
XtX_inv = solve(res$XtX) ; var_beta3_hat = t(c(0,0,0,1)) %*% XtX_inv %*% c(0,0,0,1)
t_0 = beta3_hat / sqrt(res$MSE * var_beta3_hat)
t_alpha = qt(0.025, 16, lower.tail = FALSE)
t_0 ; t_alpha
```

## 13.3 구간별 선형회귀를 적합하는 경우

```{r}
x1 = c(480, 720, 570, 300, 800, 400, 340, 650)
y = c(377, 249, 355, 475, 139, 452, 440, 257)
x2 = ifelse(x1 < 500, 0, 1)
x1_x2 = (x1 - 500) * x2 ; x1_x2
X = cbind(x1, x1_x2)

res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
res$beta_hat

```

## 13.4 반응변수가 가변수인 경우

```{r}
x1 = c(8, 22, 28, 4, 19, 13, 24, 32, 9, 13,
       20, 5, 30, 11, 30, 6, 22, 12, 18, 4,
       18, 25, 6, 29, 14)
y = c(1, 1, 1, 0, 0, 1, 0, 1, 0, 0,
      1, 0, 1, 0, 1, 0, 1, 0, 0, 0,
      1, 1, 0, 0, 0)
X = cbind(x1) ; n = dim(X)[1] ; one = c(rep(1, n))

simp_res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
simp_res$beta_hat

simp_res$MSE * solve(simp_res$XtX)

X = cbind(one, x1)
p_hat = X %*% simp_res$beta_hat
V_hat = p_hat * (1 - p_hat) ; V_hat = as.vector(V_hat)
V_hat = diag(V_hat) ; V_hat_inv = solve(V_hat)

beta_hat_gls = solve(t(X) %*% V_hat_inv %*% X) %*% t(X) %*% V_hat_inv %*% y
beta_hat_gls

solve(t(X) %*% V_hat_inv %*% X)
```

## 13.5 R 실습

### 13.5.1 가변수를 사용한 회귀모형 추정

$\textbf{Carseats 데이터}$ 이 데이터는 ISLR 패키지를 설치한 후 얻을 수 있으며, 400개 지점에서의 카시트 판매 정보를 담고 있다. `Carseats` 데이터에서 사용할 변수에 대한 설명은 다음과 같다.

- `Sales` : 판매량 (단위 : 1,000)
- `Price` : 각 지점에서의 카시트 가격
- `ShelveLoc` : 진열대의 등급 (Bad, Medium, Good)
- `Urban` : 도시 여부 (Yes, No)
- `US` : 미국 여부 (Yes, No)

위 데이터에서 `ShelveLoc`, `Urban`, `US`가 질적변수임을 알 수 있다. 판매량을 반응변수로 하여 중회귀모형을 적합한 결과를 살펴보자.

```{r}
library(ISLR)
data(Carseats)
```

```{r}
fit <- lm(Sales ~ ShelveLoc + Urban + US, data = Carseats)
summary(fit)
```

위 결과에서 보면 `ShelveLocGood, ShelveLocMedium, UrbanYes, USYes` 변수는 데이터에 없는 변수명인데 모형을 적합한 결과 자동으로 생성된 것을 확인할 수 있다. 이유는 R에서는 질적인 변수들에 대하여 가변수를 자동으로 생성하기 때문이다. 위 데이터를 살펴 보면 세 개의 변수 `ShelveLoc, Urban, US`는 factor 변수로 저장되어 있어 질적변수 임을 확인할 수 있다. 예를 들어, 범주가 3개인 `ShelveLoc`의 경우 2개의 가변수가 필요한데, 이 가변수가 `ShelveLocGood`과 `ShelveLocMedium`으로 표현되었음을 알 수 있다. 이를 확인하기 위해서는 다음과 같이 `contrasts()`함수를 사용할 수 있다. 아래 결과를 살펴보면 열에 나타난 `Good, Medium`이 `ShelveLoc`변수의 가변수에 해당하고, 각 범주에 따라 두 가변수가 어떤 값을 갖는지 보여 준다.

```{r}
contrasts(Carseats$ShelveLoc)
```

위 데이터에서 `Price, ShelveLoc, Urban, US`변수를 사용하여 `Sales`를 예측하기 위한 다중 회귀모형을 쓰면 다음과 같다.

\begin{align*}
  \text{Sales}_i = \beta_0 & + \beta_1 \text{Price}_i + \beta_2 \text{ShelveLocGood}_i + \beta_3 \text{ShelveLocMedium}_i \\
  & + \beta_4 \text{UrbanYes}_i + \beta_5 \text{USYes}_i + \epsilon_i
\end{align*}

따라서 설명변수 진열대의 등급(`ShelveLoc`)이 판매량(sales)에 영향을 미치는 변수인지 검정하기 위해서는 $H_0 : \beta_2 = \beta_3 = 0$을 검정해야 한다. 이것은 7장에서 학습한 $C\boldsymbol{\mathbf{\beta}} = 0$의 검정으로 표현하면

\begin{equation*}
  C\boldsymbol{\mathbf{\beta}} = 
  \begin{pmatrix}
    0 & 0 & 1 & 0 & 0 & 0\\
    0 & 0 & 0 & 1 & 0 & 0
  \end{pmatrix}
  \begin{pmatrix}
    \beta_0 \\ \beta_1 \\ \beta_2 \\ \beta_3 \\ \beta_4 \\ \beta_5
  \end{pmatrix}
  =
  \begin{pmatrix}
    0 \\ 0
  \end{pmatrix}
\end{equation*}

이 됨을 알 수 있고, 7장 실습에서 했던 `car`패키지의 `linearHypothesis()`함수를 사용하여 다음과 같이 검정할 수 있다.

```{r}
library(car)
C <- rbind(c(0,0,1,0,0,0),
           c(0,0,0,1,0,0))

fit <- lm(Sales ~ Price + ShelveLoc + Urban + US, data = Carseats)
summary(fit)

linearHypothesis(model = fit, hypothesis.matrix = C)
```

### 13.5.2 교호작용효과 추정

먼저 두 질적변수에 대한 교호작용효과를 알아보기 위해, 설명변수로 `Urban, US` 변수를 선택하여 적합해 보기로 한다.

```{r}
fit1 = lm(Sales ~ US * Urban, data = Carseats)
summary(fit1)
```

위 결과에서 살펴보면 교호작용의 효과는 -1.0983으로, 유의수준 5%에서 미국내에서의 판매여부(`US`)와 판매량과의 관계가 도시여부(`Urban`)에 따라 달라진다고 말할 수 없다. 교호작용효과에 대한 그림을 직접 그리기 위해 도시가 아닌 경우 (`Urban_No`)에 미국내 판매량과 미국외 판매량을 구하고, 도시인 경우 (`Urban_Yes`)에 미국내 판매량과 미국외 판매량을 구하여 `plot()`함수를 사용하여 다음과 같이 그릴 수 있다.

```{r}
coeff <- fit1$coefficients
Urban_No = matrix(0,2,1)
Urban_No[1,1] <- t(coeff) %*% c(1, 0, 0, 0) # beta_0
Urban_No[2,1] <- t(coeff) %*% c(1, 1, 0, 0) # beta_0 + beta_1

Urban_Yes = matrix(0,2,1)
Urban_Yes[1,1] <- t(coeff) %*% c(1, 0, 1, 0) # beta_0 + beta_2
Urban_Yes[2,1] <- t(coeff) %*% c(1, 1, 1, 1) # beta_0 + beta_1 + beta_2 + beta_3

plot(x = c(0, 1), y = Urban_No, type = "b", col = "red", lwd = 2,
     xlab = "US", ylab = "Sales", main = "US * Urban effect plot", xaxt = "n")
axis(side = 1, at = c(0,1), labels = c("No", "Yes"))
lines(x = c(0, 1), y = Urban_Yes, type = "b", col = "blue", lwd = 2)
legend("topleft", legend = c("Urban : No", "Urban : Yes"), col = c("red", "blue"), lwd = 2)
```

또, 교호작용효과를 적합한 회귀모형의 결과로부터 `effects` 패키지에서 `effects()`함수를 사용하면 다음과 같다.

```{r}
# install.packages("effects")
library(effects)
a = effect(term = "US * Urban", mod = fit1)
plot(a, multiline = TRUE)
```

이제 설명변수가 연속형과 질적변수인 경우의 교호작용효과에 대해 살펴보자. 이를 위해 `Price`와 `Urban`을 설명변수로 하여 교호작용이 있는 회귀모형을 적합해 보기로 한다.

```{r}
fit2 = lm(Sales ~ Price * Urban, data = Carseats)
summary(fit2)
```

위 결과에서 살펴보면 교호작용의 효과는 0.01195로, 유의수준 5%에서 카시트 가격(`Price`)과 카시트의 판매량(`Sales`)과의 관계는 도시여부(`Urban`)에 따라 달라진다고 말할 수 없다. 이번에도 교호작용효과에 대한 그림을 직접 그리기 위해 도시가 아닌 경우(`Urban_No`)와 도시인 경우(`Urban_Yes`)에 따라 가격과 판매량의 회귀식을 다음과 같이 그려 보았다.

```{r}
coeff = fit2$coefficients
x = seq(min(Carseats$Price), max(Carseats$Price), length = 10) # grid points
Urban_No = Vectorize(function(x) t(coeff) %*% c(1, x, 0, 0)) # beta_0 + beta_1 * Price
Urban_Yes = Vectorize(function(x) t(coeff) %*% c(1, x, 1, x)) # (beta_0 + beta_2) + (beta_1 + beta_3) * Price


plot(x = x, y = Urban_No(x), type = "l", col = "red", lwd = 2,
     xlab = "Price", ylab = "Sales", main = "Price * Urban effect plot", xaxt = "n")
axis(side = 1, at = c(0,1), labels = c("No", "Yes"))
lines(x = x, y = Urban_Yes(x), type = "l", col = "blue", lwd = 2)
legend("topleft", legend = c("Urban : No", "Urban : Yes"), col = c("red", "blue"), lwd = 2)
```

또, 교호작용효과를 적합한 회귀모형의 결과로부터 `effects`패키지에서 `effects()`함수를 사용하여 다음과 같이 간단하게 그려볼 수도 있다.

```{r}
a = effect(term = "Price * Urban", mod = fit2)
plot(a, multiline = TRUE)
```

## 연습문제

### 13.1

```{r}
x1 = c(151, 92, 175, 31, 104, 277, 210, 120, 290, 238,
       164, 272, 295, 68, 85, 224, 166, 305, 124, 246)
x2 = c(rep(0, 10), rep(1, 10))
y = c(17, 26, 21, 30, 22, 1, 12, 19, 4, 16,
      28, 15, 11, 38, 31, 21, 20, 13, 30, 14)

x_M = x1[1:10] ; x_F = x1[11:20] ; y_M = y[1:10] ; y_F = y[11:20]

X_M = cbind(x_M) ; X_F = cbind(x_F) ; X = cbind(x1)
n1 = length(x_M) ; n2 = length(x_F)


## 두 회귀직선의 검정
male_res = mult_reg(X_M, y_M, coeff = TRUE, alpha = 0.05)
female_res = mult_reg(X_F, y_F, coeff = TRUE, alpha = 0.05)
reduced_res = mult_reg(X, y, coeff = TRUE, alpha = 0.05)

SSE_F = male_res$SSE + female_res$SSE
SSE_R = reduced_res$SSE
df_R = n1 + n2 - 2 ; df_F = (n1 - 2) + (n2 - 2)

F_0 = ((SSE_R - SSE_F)/(df_R - df_F)) / (SSE_F / df_F)
F_alpha = qf(0.05, df_R - df_F, df_F, lower.tail = FALSE)

F_0 ; F_alpha

### 방법 1의 경우
x_M_0 = c(rep(1, n1), rep(0, n1)) ; x_F_0 = c(rep(0, n2), rep(1, n2)) ; x_M = c(x1[1:10], rep(0, n1)) ; x_F = c(rep(0, n2), x1[11:20])
X = cbind(x_M_0, x_F_0, x_M, x_F)

c1 = c(1, -1, 0, 0) ; c2 = c(0, 0, 1, -1) ; C = rbind(c1, c2) ; m = c(0, 0)
mult_test_res = mult_test(C = C, m = m, X = X, y = y, X_r = NA, y_r = NA, alpha = 0.05, method = "one", coef = FALSE)

## 두 기울기의 검정
beta_1_hat = male_res$beta_hat[2] ; beta_2_hat = female_res$beta_hat[2]
var_beta_1_hat = male_res$MSE * solve(male_res$XtX)[2,2]
var_beta_2_hat = female_res$MSE * solve(female_res$XtX)[2,2]

t_0 = (beta_1_hat - beta_2_hat)/ sqrt(var_beta_1_hat + var_beta_2_hat) ; t_0
t_alpha = qt(0.025, 16, lower.tail = FALSE) ; t_alpha

### 방법 1의 경우
X = cbind(x_M_0, x_F_0, x_M, x_F)
c1 = c(0, 0, 1, -1); C = rbind(c1) ; m = c(0)
mult_test_res = mult_test(C = C, m = m, X = X, y = y, X_r = NA, y_r = NA, alpha = 0.05, method = "one", coef = FALSE)
t_0^2 ; t_alpha^2
```

### 13.2

```{r}
x1 = c(151, 92, 175, 31, 104, 277, 210, 120, 290, 238,
       164, 272, 295, 68, 85, 224, 166, 305, 124, 246)
x2 = c(rep(0, 10), rep(1, 10))
y = c(17, 26, 21, 30, 22, 1, 12, 19, 4, 16,
      28, 15, 11, 38, 31, 21, 20, 13, 30, 14)
X = cbind(x1, x2)

mult_res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
mult_res$beta_hat

beta_1_hat = mult_res$beta_hat[2]
mult_res$MSE * solve(mult_res$XtX)
mult_res$MSE * solve(mult_res$XtX)[2,2]
var_beta_1_hat = mult_res$MSE * solve(mult_res$XtX)[2,2]
t_alpha = qt(0.025, 20 - 2 - 1, lower.tail = FALSE)

CI_lower = beta_1_hat - t_alpha * sqrt(var_beta_1_hat)
CI_upper = beta_1_hat + t_alpha * sqrt(var_beta_1_hat)

CI_lower ; CI_upper
```

### 13.3

```{r}
x1 = c(151, 92, 175, 31, 104, 277, 210, 120, 290, 238,
       164, 272, 295, 68, 85, 224, 166, 305, 124, 246)
x2 = c(rep(0, 10), rep(1, 10))
x1x2 = x1 * x2
y = c(17, 26, 21, 30, 22, 1, 12, 19, 4, 16,
      28, 15, 11, 38, 31, 21, 20, 13, 30, 14)
X = cbind(x1, x2, x1x2)

mult_res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
beta_hat = mult_res$beta_hat ; beta_hat

c1 = c(0, 1, 0, 0) ; C = rbind(c1) ; m = c(-0.3)
mult_test_res = mult_test(C = C, m = m, X = X, X_r = NA, y = y, y_r = NA, alpha = 0.1, method = "one", coef = TRUE)
```

### 13.4

```{r}
x1 = c(100,125,220,205,300,255,225,175,270,170,155,190,140,290,265)
y1 = c(218,248,360,351,470,394,332,321,410,260,241,331,275,425,367)

x2 = c(105,215,270,255,175,135,200,275,155,320,190,295)
y2 = c(140,277,384,341,215,180,260,361,252,422,273,410)

n1 = length(x1) ; n2 = length(x2)
x1 = c(x1, x2) ; y = c(y1, y2)
x2 = c(rep(1, n1), rep(0, n2))
X = cbind(x1, x2)
```

#### 13.4 - (1)

```{r}
mult_res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
beta_hat = mult_res$beta_hat ; beta_hat

## beta1의 신뢰구간
beta1_hat = beta_hat[2]
var_beta1_hat = mult_res$MSE * solve(mult_res$XtX)[2,2]
t_alpha = qt(0.025, n1 + n2 - 3, lower.tail = FALSE)

beta1_CI_lower = beta1_hat - t_alpha * sqrt(var_beta1_hat) 
beta1_CI_upper = beta1_hat + t_alpha * sqrt(var_beta1_hat)
beta1_CI_lower ; beta1_CI_upper

## beta2의 신뢰구간
beta2_hat = beta_hat[3]
var_beta2_hat = mult_res$MSE * solve(mult_res$XtX)[3,3]
t_alpha = qt(0.025, n1 + n2 - 3, lower.tail = FALSE)

beta2_CI_lower = beta2_hat - t_alpha * sqrt(var_beta2_hat)
beta2_CI_upper = beta2_hat + t_alpha * sqrt(var_beta2_hat)
beta2_CI_lower ; beta2_CI_upper
```

#### 13.4 - (2)

```{r}
x1x2 = x1 * x2
X = cbind(x1, x2, x1x2)

mult_res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
beta_hat = mult_res$beta_hat ; beta_hat

## beta2의 신뢰구간
beta2_hat = beta_hat[3]
var_beta1_hat = mult_res$MSE * solve(mult_res$XtX)[3,3]
t_alpha = qt(0.025, n1 + n2 - 4, lower.tail = FALSE)

beta2_CI_lower = beta2_hat - t_alpha * sqrt(var_beta2_hat) 
beta2_CI_upper = beta2_hat + t_alpha * sqrt(var_beta2_hat)
beta2_CI_lower ; beta2_CI_upper

## beta3의 신뢰구간
beta3_hat = beta_hat[4]
var_beta3_hat = mult_res$MSE * solve(mult_res$XtX)[4,4]
t_alpha = qt(0.025, n1 + n2 - 4, lower.tail = FALSE)

beta3_CI_lower = beta3_hat - t_alpha * sqrt(var_beta3_hat)
beta3_CI_upper = beta3_hat + t_alpha * sqrt(var_beta3_hat)
beta3_CI_lower ; beta3_CI_upper
```

### 13.5

```{r}
x1 = c(151, 92, 175, 31, 104, 277, 210, 120, 290, 238,
       164, 272, 295, 68, 85, 224, 166, 305, 124, 246)
x2 = c(rep(0, 10), rep(1, 10)) # 남자가 0, # 여자가 1
x3 = c(0, 1, 0, 1, 1, 0, 0, 0, 0, 0,
       0, 0, 0, 1, 0, 0, 0, 0, 0, 0) # 고등학교졸이 1, 그외는 0
x4 = c(1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
       0, 0, 0, 0, 1, 0, 1, 0, 1, 0) # 대학교졸이 1, 그외는 0
y = c(17, 26, 21, 30, 22, 1, 12, 19, 4, 16,
      28, 15, 11, 38, 31, 21, 20, 13, 30, 14)
X = cbind(x1,x2,x3,x4)
```

#### 13.5 - (1), (2)

```{r}
mult_res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
beta_hat = mult_res$beta_hat ; beta_hat
```

#### 13.5 - (3)

```{r}
## beta3의 신뢰구간
beta3_hat = beta_hat[4]
var_beta3_hat = mult_res$MSE * solve(mult_res$XtX)[4,4] ; var_beta3_hat
t_alpha = qt(0.025, 20 - 5, lower.tail = FALSE)

beta3_CI_lower = beta3_hat - t_alpha * sqrt(var_beta3_hat)
beta3_CI_upper = beta3_hat + t_alpha * sqrt(var_beta3_hat)
beta3_CI_lower ; beta3_CI_upper

## beta4의 신뢰구간
beta4_hat = beta_hat[5]
var_beta4_hat = mult_res$MSE * solve(mult_res$XtX)[5,5] ; var_beta4_hat
t_alpha = qt(0.025, 20 - 5, lower.tail = FALSE)

beta4_CI_lower = beta4_hat - t_alpha * sqrt(var_beta4_hat)
beta4_CI_upper = beta4_hat + t_alpha * sqrt(var_beta4_hat)
beta4_CI_lower ; beta4_CI_upper
```

#### 13.5 - (4)

```{r}
x1x2 = x1 * x2 ; x1x3 = x1 * x3 ; x1x4 = x1 * x4 ; x2x3 = x2 * x3 ; x2x4 = x2 * x4
X = cbind(x1, x2, x3, x4, x1x2, x1x3, x1x4, x2x3, x2x4)

mult_res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
beta_hat = mult_res$beta_hat ; beta_hat

c1 = c(0, 0, 0, 0, 0, 1, 0, 0, 0, 0)
c2 = c(0, 0, 0, 0, 0, 0, 1, 0, 0, 0)
c3 = c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0)
c4 = c(0, 0, 0, 0, 0, 0, 0, 0, 1, 0)
c5 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1)

C = rbind(c1, c2, c3, c4, c5) ; m = c(0, 0, 0, 0, 0)

mult_test_res = mult_test(C, m, X, NA, y, NA, alpha = 0.1, method = "one", coef = TRUE)

# qf(0.1, 5, 10, lower.tail = FALSE)
```

### 13.6

```{r}
x = c(1,2,3,4,5,6,7) ; y = c(2.0, 3.2, 4.1, 5.2, 7.0, 9.7, 11.5)
```

#### 13.6 - (1)

```{r}
plot(x, y, xlab = "x", ylab = "y", cex = 1.0, main = "x y 산점도")
```

#### 13.6 - (2)

```{r}
x1 = x
x2 = ifelse(x1 >= 4, 1, 0) ; x2
x1x2 = (x1 - 4) * x2 ; x1x2

X = cbind(x1, x1x2)

mult_res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
beta_hat = mult_res$beta_hat ; beta_hat
```

#### 13.6 - (3)

```{r}
## beta2의 신뢰구간
beta2_hat = beta_hat[3]
var_beta2_hat = mult_res$MSE * solve(mult_res$XtX)[3,3] ; var_beta2_hat
t_alpha = qt(0.05, 7 - 3, lower.tail = FALSE)

beta2_CI_lower = beta2_hat - t_alpha * sqrt(var_beta2_hat)
beta2_CI_upper = beta2_hat + t_alpha * sqrt(var_beta2_hat)
beta2_CI_lower ; beta2_CI_upper
```

```{r}

```

```{r}
x1 <- seq(1, 4, length.out = 100)
x2 <- seq(4, 7, length.out = 100)

plot(x = x, y = y, xlab = "x", ylab = "y",
     main = "조각별 다항회귀", col = "black", bg = "black", pch = 21, cex = 1, xlim = c(1, 7), ylim = c(0, 14))

## 첫 번째 구간 예측 선 (x in [1,4])
x1 <- seq(1, 4, length.out = 100)
y1 <- mult_res$beta_hat[1] + mult_res$beta_hat[2] * x1
lines(x1, y1, col = "lightblue", lwd = 2)

## 두 번째 구간 예측 선 (x in [4,7])
x2 <- seq(4, 7, length.out = 100)
# y2 <- mult_res$beta_hat[1] + mult_res$beta_hat[2] * x2 + mult_res$beta_hat[3] * (x2 - 4)
y2 <- (mult_res$beta_hat[1] - 4 * mult_res$beta_hat[3]) + (mult_res$beta_hat[2] + mult_res$beta_hat[3]) * x2
lines(x2, y2, col = "blue", lwd = 2)
```

### 13.7

```{r}
x = c(410, 500, 471, 619, 584, 681, 399, 481, 624, 589)
y = c(0, 1, 0, 1, 0, 1, 0, 1, 1, 1)
X = cbind(x) ; n = length(x) ; one = c(rep(1, n))
```

#### 13.7 - (1)

```{r}
res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)
beta_hat = res$beta_hat ; beta_hat
```

#### 13.7 - (2)

```{r}
X = cbind(one, x)
p_hat = X %*% beta_hat
var_p_hat = p_hat * (1 - p_hat) ; var_p_hat = as.vector(var_p_hat)

V_hat = diag(var_p_hat)

V_hat_inv = solve(V_hat)

beta_hat_gls = solve(t(X) %*% V_hat_inv %*% X) %*% t(X) %*% V_hat_inv %*% y
beta_hat_gls
```

#### 13.7 - (3)

```{r}
res$MSE * solve(res$XtX)[2,2]

solve(t(X) %*% V_hat_inv %*% X)[2,2]
```

#### 13.7 - (4)

```{r}
y_hat = t(c(1, 550)) %*% beta_hat_gls ; y_hat
```

### 13.8

```{r}
data(pbc, package = "survival")
unique(pbc$status)
unique(pbc$sex)
unique(pbc$trt)

library(dplyr)
dataset = pbc %>% na.omit() %>% select(status, age, sex, trt)

dim(pbc)
dim(dataset)
unique(dataset$status)

dataset = dataset[ dataset$status != 1, ]

dim(dataset)
```

```{r}
unique(dataset$status)
unique(dataset$sex)
unique(dataset$trt)
```

환자의 상태(status) 변수가 반응변수이고, 이때 반응변수는 0(이식을 받지 못한 경우), 2(사망한 경우)의 총 두가지 경우만 있으므로 이진변수로 볼 수 있다. 따라서 2를 1로 바꿔도 무방하다. (즉 1이 사망한 경우를 의미) 성별의 경우 남자는 1, 여자는 0으로 두어도 무방하다. 치료변수의 경우 치료약을 처방받은 경우를 1로 그대로 두고, 위약을 처방받은 경우를 2에서 0으로 바꾸어도 된다.

```{r}
dataset$status = ifelse(dataset$status == 2, 1, 0)
dataset$sex = ifelse(dataset$sex == "f", 0, 1)
dataset$trt = ifelse(dataset$trt == 2, 0, 1)
```

```{r}
unique(dataset$status)
unique(dataset$sex)
unique(dataset$trt)
```

```{r}
X = dataset[,-c(1)] ; y = dataset[,c(1)]
X = as.matrix(X)
res = mult_reg(X, y, alpha = 0.05, coeff = TRUE)

beta_hat = res$beta_hat ; beta_hat
```

우선 적합된 회귀모형은 다음과 같다 :

\begin{equation*}
  \widehat{\text{status}_i} = -0.0908 + 0.0098 \times \text{age}_i + 0.1968 \times \text{sex}_i + 0.0015 \times \text{trt}_i
\end{equation*}

이때 유의수준 $\alpha = 0.05$에서 $F_0 = 6.810953	> F_{\alpha}(3, 254) = 2.64014$ 으로 귀무가설 $H_0 : \beta_1 = \beta_2 = \beta_3 = 0$은 기각된다. 즉 회귀모형은 유의수준 $\alpha = 0.05$에서 유의하다고 할 수 있다.

참고 : 각 회귀계수의 기울기가 유의한지에 대한 검정은 7장의 방법1을 통해서도 할 수 있다. 

```{r}
c1 = c(0, 1, 0, 0) ; c2 = c(0, 0, 1, 0) ; c3 = c(0, 0, 0, 1)
C = rbind(c1, c2, c3) ; m = c(0, 0, 0)
mult_test_res = mult_test(C, m, X, NA, y, NA, alpha = 0.05, method = "one", coef = TRUE)
```

그런데 반응변수가 가변수이고, 이진변수이므로, 반응변수 `status`에 대하여 베르누이 확률분포를 가정할 수 있다. 따라서

\begin{equation*}
  E(y_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_3 x_{i3} = p_i
\end{equation*}

으로 둘 수 있다. 교재의 내용대로 일반화최소제곱추정방법을 사용하면

```{r}
n = dim(X)[1] ; one = c(rep(1, n))
p_hat = cbind(one, X) %*% beta_hat
var_p_hat = p_hat * (1 - p_hat) ; var_p_hat = as.vector(var_p_hat)

V_hat = diag(var_p_hat) ; V_hat_inv = solve(V_hat)

beta_hat_gls = solve(t(cbind(one, X)) %*% V_hat_inv %*% cbind(one, X)) %*% t(cbind(one, X)) %*% V_hat_inv %*% y
beta_hat_gls
```

따라서 이 경우 적합된 회귀모형은 다음과 같다.

\begin{equation*}
  \widehat{\text{status}_i} = - 0.0716 + 0.0095 \times \text{age}_i + 0.1729 \times \text{sex}_i - 0.0040 \times \text{trt}_i
\end{equation*}

각 변수의 교호작용관계가 존재하는지를 확인해보는 절차는 다음과 같다.

```{r}
age_sex = X[,"age"] * X[,"sex"]
age_trt = X[,"age"] * X[,"trt"]
sex_trt = X[,"sex"] * X[,"trt"]

X = cbind(X, age_sex, age_trt, sex_trt)

c1 = c(0, 0, 0, 0, 1, 0, 0)
c2 = c(0, 0, 0, 0, 0, 1, 0)
c3 = c(0, 0, 0, 0, 0, 0, 1)
C = rbind(c1, c2, c3) ; m = c(0, 0, 0)

mult_test_res = mult_test(C, m, X, NA, y, NA, alpha = 0.05, method = "one", coef = TRUE)
```

이때 $F_0 = 1.7984 < F_{\alpha}(3, 251) = 2.640564$ 으로, 귀무가설 $H_0 : \beta_i = 0 ~~(i = 4,5,6)$은 유의수준 $\alpha = 0.05$에서 기각되지 못한다. 즉 각 변수의 교호작용관계가 존재한다고 할 수 있다.